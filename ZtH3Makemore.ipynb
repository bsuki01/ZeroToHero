{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0463d7df-ed6b-451e-bac4-b9fd289e0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8139f24-655f-4505-b613-8c2dd27062f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]\n",
    "#Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677a09ba-fad5-49d7-86a6-5e934f8faf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315eaf55-a351-4ad1-bcc7-d1bd5eb831f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0 # character key, integer value (dictionary)\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "#build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "078933ad-0310-454f-a415-a30561a338aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "    \n",
    "        # print(w)\n",
    "        context = [0] * block_size \n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(.8*len(words))\n",
    "n2 = int(.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "#build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cec18501-efec-442b-9e0f-35ed3ad61fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n"
     ]
    }
   ],
   "source": [
    "#MLP:\n",
    "n_embd = 10 #dimensionality of table C\n",
    "n_hidden = 200 #number of neurons\n",
    "\n",
    "# experiment: initialize weights and biases to zero\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)# * 0.0001\n",
    "# b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g)# * 0.0001\n",
    "b2 = torch.randn(vocab_size,                      generator=g)# * 0.0001\n",
    "\n",
    "#batchnorm: parameters\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67317791-d859-4d63-bc35-fda2b308105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "W1.grad: None\n",
      "W2.grad: None\n",
      "hpreact: tensor([[-7.8105,  2.9366, -0.1749,  ..., -0.4312,  6.6466,  6.9614],\n",
      "        [-2.4405, -1.9209, -8.6752,  ..., -5.3048,  2.2211,  2.4768],\n",
      "        [ 4.8504, -1.6981, -6.1383,  ..., -5.5880, -0.5597,  4.3928],\n",
      "        ...,\n",
      "        [ 0.1639,  0.8871, -2.1176,  ..., -6.7441,  6.3849, 11.6734],\n",
      "        [-7.8105,  2.9366, -0.1749,  ..., -0.4312,  6.6466,  6.9614],\n",
      "        [ 1.0001,  4.9044, -2.7879,  ..., -7.0093,  7.9342,  3.5855]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "      0/ 200000: 20.8244\n",
      "  10000/ 200000: 2.2621\n",
      "  20000/ 200000: 2.7875\n",
      "  30000/ 200000: 2.1976\n",
      "  40000/ 200000: 2.4359\n",
      "Step 50000\n",
      "W1.grad: tensor([[-9.4260e-03, -6.0207e-03, -6.5543e-04,  ..., -3.4276e-03,\n",
      "          8.0746e-05, -4.2116e-04],\n",
      "        [ 3.7547e-03, -6.1412e-04,  3.4558e-04,  ...,  6.2035e-04,\n",
      "          7.5797e-04,  1.0704e-03],\n",
      "        [-2.0198e-03, -1.3690e-03,  7.0697e-05,  ...,  4.4503e-04,\n",
      "          4.2927e-04,  8.5544e-04],\n",
      "        ...,\n",
      "        [ 5.7858e-03,  4.3070e-04, -1.4362e-03,  ...,  1.4546e-02,\n",
      "          3.0168e-03, -2.9036e-04],\n",
      "        [-2.1469e-03,  3.8583e-03,  4.0799e-04,  ..., -6.0629e-03,\n",
      "         -2.1712e-04, -1.9609e-03],\n",
      "        [ 1.4487e-03,  1.1647e-03, -5.0080e-04,  ...,  5.1842e-03,\n",
      "          7.4317e-04, -5.0333e-04]])\n",
      "W2.grad: tensor([[ 0.0344,  0.0261,  0.0216,  ..., -0.0013, -0.0044, -0.0047],\n",
      "        [ 0.0304,  0.0081, -0.0259,  ...,  0.0002, -0.0075,  0.0011],\n",
      "        [-0.0066, -0.0084, -0.0052,  ...,  0.0002,  0.0029,  0.0007],\n",
      "        ...,\n",
      "        [-0.0298,  0.0220,  0.0223,  ..., -0.0002, -0.0140, -0.0005],\n",
      "        [ 0.0329, -0.0102, -0.0142,  ...,  0.0002,  0.0056,  0.0008],\n",
      "        [ 0.0169, -0.0006, -0.0195,  ...,  0.0001,  0.0038,  0.0007]])\n",
      "hpreact: tensor([[  0.5662,  -5.2924, -20.6834,  ...,   6.5349,  -6.5182,  -7.0867],\n",
      "        [ -2.6901,  -2.6948, -17.1883,  ...,   1.4428,  -2.0563,  -1.3252],\n",
      "        [ -4.4406,   1.3739, -13.0588,  ...,  12.0912,  -7.2993,  -4.8291],\n",
      "        ...,\n",
      "        [ -0.0618,  -8.8349,  -5.9049,  ...,  -0.6640,  -4.8945,  -6.9641],\n",
      "        [  2.6649,  -4.3281,   2.4242,  ...,   4.0631, -15.3985,  -6.1754],\n",
      "        [  3.7146,  -3.9612,  -3.2262,  ...,   2.7564, -10.7605,  -9.9292]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "  50000/ 200000: 1.8465\n",
      "  60000/ 200000: 2.4030\n",
      "  70000/ 200000: 2.3083\n",
      "  80000/ 200000: 2.6540\n",
      "  90000/ 200000: 2.3302\n",
      "Step 100000\n",
      "W1.grad: tensor([[ 1.1324e-03,  1.0916e-02,  7.5599e-06,  ...,  1.0238e-03,\n",
      "          1.4430e-04,  1.7584e-03],\n",
      "        [ 1.2870e-03, -1.0350e-03,  7.9213e-06,  ...,  3.8817e-04,\n",
      "          2.2613e-05, -2.3810e-05],\n",
      "        [ 1.5292e-03,  7.5576e-04, -8.2496e-06,  ..., -1.2584e-03,\n",
      "          1.5616e-04,  3.0702e-04],\n",
      "        ...,\n",
      "        [ 2.4413e-03, -4.9589e-03, -5.3796e-05,  ..., -1.1150e-03,\n",
      "          2.2712e-03, -8.1839e-04],\n",
      "        [-2.4316e-03, -1.3623e-03, -1.3927e-05,  ...,  2.5629e-03,\n",
      "         -4.4049e-04,  1.9697e-04],\n",
      "        [ 7.7835e-05,  2.4503e-03, -3.4564e-05,  ...,  6.5459e-04,\n",
      "          1.1006e-03,  3.1001e-04]])\n",
      "W2.grad: tensor([[ 7.6705e-02, -1.6269e-02,  1.8792e-02,  ..., -1.9342e-03,\n",
      "          1.7000e-02, -9.8618e-03],\n",
      "        [ 3.3571e-02,  1.1870e-02, -2.1132e-02,  ..., -7.6275e-04,\n",
      "         -1.0480e-02,  4.3873e-03],\n",
      "        [ 1.6733e-04,  2.3811e-04, -2.6213e-04,  ...,  1.2928e-05,\n",
      "         -4.1248e-04,  8.0955e-05],\n",
      "        ...,\n",
      "        [-7.3621e-03, -1.1742e-02,  1.6062e-02,  ...,  9.6069e-05,\n",
      "          1.2079e-02, -4.7440e-03],\n",
      "        [-9.8095e-04,  1.1809e-02, -7.7073e-03,  ...,  1.0265e-04,\n",
      "         -3.1253e-03,  2.4783e-03],\n",
      "        [ 2.2385e-02,  1.4280e-04, -9.7360e-03,  ..., -4.0875e-04,\n",
      "         -1.1471e-02,  2.3313e-03]])\n",
      "hpreact: tensor([[  4.0008,  -3.2449,   0.2743,  ...,   1.6350, -13.6260,  -8.8556],\n",
      "        [-10.5141,  17.4445,   0.3341,  ..., -18.7922,  13.5987,  23.5313],\n",
      "        [  1.0456,  -8.6356,  -3.1660,  ...,  11.6018, -11.7273,  -9.4599],\n",
      "        ...,\n",
      "        [-10.5141,  17.4445,   0.3341,  ..., -18.7922,  13.5987,  23.5313],\n",
      "        [  0.6102,  -8.6578, -16.3429,  ...,  13.5734, -11.4294, -10.5332],\n",
      "        [  4.9625,  -5.0987, -10.9153,  ...,  -8.1463,   0.6356,   5.8937]],\n",
      "       grad_fn=<MmBackward0>)\n",
      " 100000/ 200000: 2.3849\n",
      " 110000/ 200000: 2.4087\n",
      " 120000/ 200000: 1.7559\n",
      " 130000/ 200000: 2.0586\n",
      " 140000/ 200000: 2.2154\n",
      "Step 150000\n",
      "W1.grad: tensor([[ 5.0497e-03,  4.8152e-03, -1.3632e-05,  ...,  2.7684e-03,\n",
      "         -3.5261e-04,  1.0406e-03],\n",
      "        [-6.1676e-04, -3.2437e-03, -5.1695e-07,  ...,  3.2669e-03,\n",
      "          1.3605e-04, -1.1126e-03],\n",
      "        [ 3.6067e-04,  3.9473e-04, -1.3066e-05,  ...,  9.9101e-04,\n",
      "         -1.9991e-04,  1.2491e-04],\n",
      "        ...,\n",
      "        [-3.6490e-03, -1.0899e-03, -1.1712e-07,  ...,  8.7733e-04,\n",
      "          1.5764e-05, -1.8406e-03],\n",
      "        [ 3.9287e-04,  1.0671e-03,  1.5496e-05,  ..., -2.3352e-03,\n",
      "         -3.9905e-04, -6.9744e-05],\n",
      "        [ 7.0873e-03,  8.7386e-03, -1.4343e-05,  ...,  3.9109e-03,\n",
      "          4.9566e-04,  6.3416e-04]])\n",
      "W2.grad: tensor([[ 4.9071e-03,  1.4210e-02,  4.1396e-02,  ..., -1.1969e-03,\n",
      "         -3.8179e-03, -1.0885e-02],\n",
      "        [-7.0876e-03,  1.4577e-02, -6.0004e-03,  ...,  4.3158e-04,\n",
      "          1.8701e-02,  7.7952e-03],\n",
      "        [-1.6627e-04, -2.3739e-04, -1.2951e-04,  ...,  5.7960e-06,\n",
      "          1.1185e-05,  5.1689e-05],\n",
      "        ...,\n",
      "        [-1.2563e-03, -5.3296e-03,  2.7896e-03,  ..., -6.8926e-04,\n",
      "         -2.1109e-02, -7.1427e-03],\n",
      "        [ 3.4468e-03,  1.4206e-03, -3.3061e-03,  ...,  3.3565e-04,\n",
      "          1.0477e-02,  3.0314e-03],\n",
      "        [ 4.3869e-04,  7.0485e-03,  1.6130e-03,  ...,  2.2249e-04,\n",
      "          7.9567e-03,  3.8982e-03]])\n",
      "hpreact: tensor([[  0.6918, -15.4137,  -8.7921,  ...,   1.7721,  -6.3662, -10.1229],\n",
      "        [  7.3731,  -9.1926,   0.3909,  ...,   0.5234,  -3.1575, -11.2147],\n",
      "        [  6.8990, -18.5602,  -7.3255,  ...,  12.6291, -12.0526, -23.3108],\n",
      "        ...,\n",
      "        [-10.7823,  17.3792,   0.4576,  ..., -18.8958,  13.6147,  23.5054],\n",
      "        [ -4.3086,   4.4653,  -1.4990,  ...,  -1.2621,  -1.9176,   2.0808],\n",
      "        [  0.4879,  -6.7052,  15.8680,  ...,   3.6261,  -4.3524,  -4.4682]],\n",
      "       grad_fn=<MmBackward0>)\n",
      " 150000/ 200000: 2.1320\n",
      " 160000/ 200000: 2.1069\n",
      " 170000/ 200000: 2.4335\n",
      " 180000/ 200000: 2.1928\n",
      " 190000/ 200000: 2.2798\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "    #linear layer\n",
    "    hpreact = embcat @ W1 #+ b1 #hidden layer pre-activation\n",
    "    # inspect before batchnorm\n",
    "    if i % 50000 == 0:\n",
    "        print(f\"Step {i}\")\n",
    "        print(\"W1.grad:\", W1.grad)\n",
    "        print(\"W2.grad:\", W2.grad)\n",
    "        print(\"hpreact:\", hpreact)\n",
    "        # print(\"logits:\", logits)\n",
    "        \n",
    "    #______________________________________\n",
    "    #batch norm layer\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / (bnstdi + 1e-5) + bnbias #batch normalization\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + .001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnstd_running + .001 * bnstdi\n",
    "    #_____________________________________\n",
    "    #non linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    # break\n",
    "\n",
    "#gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19ec7913-fa93-47bc-b461-3cf67265b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpreact.std(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c12c5a4-9df3-43ae-9bdb-dffed624db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpreact.mean(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "beae3db0-4e81-4293-a83c-cbb7038a227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a845852a-6291-45d5-923e-9dd2552e0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibrate batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    #pass dataset thru\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 #+b1\n",
    "    #measure mean / std over entire set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53562cd9-ca00-4eb1-884b-e40f34f4e69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.194366931915283\n",
      "val 2.200974464416504\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    emb = C[x] #(N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) #concat into emb\n",
    "    hpreact = embcat @ W1# + b1 #hidden layer pre-activation\n",
    "    # hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias \n",
    "    hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact) #(N, n_hidden)\n",
    "    logits = h @ W2 + b2 #(N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "#calculate train and dev losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e11eb3-d4f7-429d-ac64-6e482a6cc6bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3152653017.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    train 2.124538\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#PREVIOUS LOSSES:\n",
    "\n",
    "#og:\n",
    "train 2.124538\n",
    "val 2.16819\n",
    "\n",
    "#fix initial run (softmax):\n",
    "train 2.07\n",
    "val 2.13\n",
    "\n",
    "#fix tanh saturation\n",
    "train 2.03559\n",
    "val 2.102678\n",
    "\n",
    "# add batch normalization layer\n",
    "train 2.0668270587921143\n",
    "val 2.104844808578491\n",
    "\n",
    "# 1/26/2024 experiment\n",
    "train 2.0674\n",
    "val 2.1056\n",
    "# gradients to 0.01\n",
    "train 2.05856\n",
    "val 2.10529\n",
    "#gradients to 0.0001\n",
    "train 2.059\n",
    "val 2.1048\n",
    "# no gradients\n",
    "train\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a0110-e761-4578-bf1c-c8b5f3a2cdad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sample model:\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647+10)\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        #forward pass\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 +b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        #sampling from distribution\n",
    "        ix = torch.multinomial(probs, num_samples = 1, generator=g).item()\n",
    "\n",
    "        #shift context window\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        #next word\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2a616-2e10-4e85-b2ad-e5bf201fbfeb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# understanding weight initiliazation\n",
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200) / 10**0.5\n",
    "y = x @ w\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1).tolist(), 50, density=True)\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1).tolist(), 50, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb8ce3-1f2e-4124-914b-4506778eee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# Pytorchifying the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebda44-8081-4b7e-8abb-4996e3d6341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# layers = [\n",
    "#   Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "# # ]\n",
    "layers = [\n",
    "Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "Linear(           n_hidden, n_hidden), Tanh(),\n",
    "Linear(           n_hidden, n_hidden), Tanh(),\n",
    "Linear(           n_hidden, n_hidden), Tanh(),\n",
    "Linear(           n_hidden, n_hidden), Tanh(),\n",
    "Linear(           n_hidden, vocab_size),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  # layers[-1].gamma *= 0.1\n",
    "  layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d12dd-ee8c-4f44-b73f-49a89a626ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for layer in layers:\n",
    "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  with torch.no_grad():\n",
    "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "  if i >= 1000:\n",
    "    break # AFTER_DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4e499-00e6-4765-b15c-b7058e20ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a034e-b6b5-4600-9e3e-9a0bcad00a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out.grad\n",
    "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04a76c-e69c-40bd-82aa-612f485b006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  t = p.grad\n",
    "  if p.ndim == 2:\n",
    "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845d64c-9ad0-495a-af5b-a12178ea507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2b3f0-0107-466c-ace6-7691ac036ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
